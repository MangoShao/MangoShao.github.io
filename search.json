[{"title":"plan","url":"/2021/04/19/plan/","content":"工作清单：- Steepest Descent在Optimization里详细介绍- 优化器收敛性简介- momentum类简介- adaptive类简介\n\n\n\n202104\n4.20\nSGD\n简介SGD，最速下降法，batch GD，问题\n\n\n\n\n4.21\n\n","categories":["Plan"]},{"title":"LeetCode 01 Solution","url":"/2021/04/20/LeetCode/01/","content":"法一：暴力class Solution:    def twoSum(self, nums: List[int], target: int) -&gt; List[int]:        lens = len(nums)        for i in range(lens):            j = i + 1            while j &lt; lens:                if nums[i] + nums[j] == target:                    return [i, j]                else:                    j += 1        return None\n\n\n空间：\n时间：\n\n法二：哈希表class Solution:    def twoSum(self, nums: List[int], target: int) -&gt; List[int]:        hashtab = {}        for idx, num in enumerate(nums):            other_num = target - num            if other_num in hashtab:                return [hashtab[other_num], idx]            hashtab[num] = idx        return None\n\n\n空间：\n时间：\n\n","categories":["LeetCode"],"tags":["TwoSum | HashList"]},{"title":"LeetCode 02 Solution","url":"/2021/04/20/LeetCode/02/","content":""},{"title":"Optimizers in Machine Learning: SGD","url":"/2021/04/19/Optimization_in_ML/SGD/","content":"Stochastic Gradient Descent (SGD) 是机器学习与深度学习中最著名也是最基础的优化器，下面介绍SGD的发展：\n优化算法的框架考虑一个无约束优化问题：，目标是寻求最优的使得最小。该问题的一个通用迭代格式如下：其中是步长，是方向。如果满足，便称为下降方向。注意，在不同算法中，不一定是下降方向。\n选择步长的方法很多，最常见的为固定步长和line search；方向的不同选择方式，可以发展出不同的优化算法，如Steepest Descent，Newton Method，Conjugate Gradient Method等。\nSteepest DescentSGD的前身就是优化中最基础的first-order算法：Steepest Descent (最速下降法)。最速下降法的motivation非常简单，选择当前下降最快的方向—负梯度，作为下降方向，其迭代格式如下：但是最速下降法的收敛速度较慢，是次线性收敛，所以在传统优化中很少使用。不过，这不影响SGD在机器学习与深度学习中大放异彩。\n在机器学习与深度学习中，最速下降法被称为Batch Gradient Descent 。\nStochastic Gradient Descent与Batch Gradient Descent相对应的便是Stochastic Gradient Descent，其迭代格式如下：其中是一个样本点。\n以MSE Loss来举例：其中，。SGD的做法便是，对于每一个样本点，都做一次对应的更新：\nSGD实际上给梯度引入了方差，理论上收敛性是次线性收敛，不过收敛速度上不会快于Batch Gradient Descent。不过，有研究认为引入随机梯度使得SGD在能比BGD更好地逃离local minima。\nSGD是每次使用一个样本点去更新，而BGD实际上选择的是全部样本，因此考虑到机器学习与深度学习问题的计算需求，一个折中的mini-batch方式，成为了这些领域的主流优化算法。\nMini-Batch Gradient DescentMini-batch是SGD与BGD的折中选择，每次更新使用一个算力允许范围内较大的batch数据。较于SGD，可以充分利用算力的同时，每次下降得更有效率，减少SGD大方差带来的不稳定性；同时相比于BGD，更好地适应了算力，不会出现算力不足而无法进行计算的尴尬情形。Mini-Batch Gradient Descent的迭代格式如下：\n值得注意的是，机器学习与深度学习中常说的SGD，一般是指Mini-Batch Gradient Descent。\nSGD的缺点SGD主要有两个方面的缺点：\n\n负梯度方向不是一个好的下降方向：使用负梯度下降，一般只能达到次线性收敛；\nscale invariant；\n\n因此，对SGD进行改进也主要分为两大方向：\n\n改进下降方向：\n第一种为Momentum类的方法：Momentum，NAG等；\n第二种便是second-order方法：Newton法，Quasi Newton法等；\n\n\n改进步长：\nAdaptive Methods：AdaGrad，RMSprop，Adam-type等。\n\n\n\n","categories":["Optimizers in ML"],"tags":["Optimization | Machine Learning"]}]