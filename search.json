[{"title":"plan","url":"/2021/04/19/plan/","content":"工作清单：- Steepest Descent在Optimization里详细介绍- 优化器收敛性简介- momentum类简介- adaptive类简介\n\n\n\n202104\n4.20\nSGD\n简介SGD，最速下降法，batch GD，问题\n\n\n\n\n4.21\n\n","categories":["Plan"]},{"title":"LeetCode 01 Solution","url":"/2021/04/20/LeetCode/01/","content":"法一：暴力class Solution:    def twoSum(self, nums: List[int], target: int) -&gt; List[int]:        lens = len(nums)        for i in range(lens):            j = i + 1            while j &lt; lens:                if nums[i] + nums[j] == target:                    return [i, j]                else:                    j += 1        return None\n\n\n空间：\n时间：\n\n法二：哈希表class Solution:    def twoSum(self, nums: List[int], target: int) -&gt; List[int]:        hashtab = {}        for idx, num in enumerate(nums):            other_num = target - num            if other_num in hashtab:                return [hashtab[other_num], idx]            hashtab[num] = idx        return None\n\n\n空间：\n时间：\n\n","categories":["LeetCode"],"tags":["TwoSum | HashList"]},{"title":"LeetCode 02 Solution","url":"/2021/04/20/LeetCode/02/","content":""},{"title":"Optimizers in Machine Learning: SGD","url":"/2021/04/19/Optimization_in_ML/SGD/","content":"Stochastic Gradient Descent (SGD) 是机器学习与深度学习中最著名也是最基础的优化器，下面介绍SGD的发展：\n优化算法的框架考虑一个无约束优化问题：，目标是寻求最优的使得最小。该问题的一个通用迭代格式如下：其中是步长，是方向。如果满足，便称为下降方向。注意，在不同算法中，不一定是下降方向。\n选择步长的方法很多，最常见的为固定步长和line search；方向的不同选择方式，可以发展出不同的优化算法，如Steepest Descent，Newton Method，Conjugate Gradient Method等。\nSteepest DescentSGD的前身就是优化中最基础的算法：Steepest Descent (最速下降法)。最速下降法的motivation非常简单，选择当前下降最快的方向—负梯度，作为下降方向，其迭代格式如下：但是最速下降法的收敛速度较慢，是次线性收敛，所以在传统优化中很少使用。不过，这不影响SGD在机器学习与深度学习中大放异彩。\nStochastic Optimization ProblemStochastic Gradient Descent","categories":["Optimizers in ML"],"tags":["Optimization | Machine Learning"]}]